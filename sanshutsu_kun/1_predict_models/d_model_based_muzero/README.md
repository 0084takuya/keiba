# 競馬 × 深層強化学習 各手法と戦略まとめ

## 1. 値関数型（Value-based）  
**例：DQN**

- **戦略例**
  - 馬券の種類や買い方（単勝・複勝・馬連など）を「行動」として定義
  - 各レースの情報（馬の成績、オッズ、天候など）を「状態」として入力
  - 「どの馬券を買うと最も利益が出るか」をQ値で予測し、最適な馬券を選ぶ
- **ポイント**
  - 過去のデータから「この状況ならこの馬券が儲かる」というパターンを学習
  - 行動（馬券の選択）が離散的な場合に向いている

- **推奨手法：DQN（Deep Q-Network）**
  - **理由**  
    - 馬券の種類（単勝・複勝・馬連など）を「行動」として選択しやすい
    - 過去のレースデータから「どの馬券が最も利益を生むか」を学習できる
    - 行動が離散的な場合（例：どの馬券を買うか）に特に有効

---

## 2. 方策型（Policy-based）  
**例：REINFORCE, A2C**

- **戦略例**
  - 馬券の購入金額や組み合わせを「連続的」に調整する戦略
  - 「どの馬券をどれだけ買うか」を直接ニューラルネットワークで出力
  - 利益が最大化するように方策（馬券購入ルール）を学習
- **ポイント**
  - 賭け金の配分や複雑な買い方（例：資金配分馬券）に強い
  - 連続的な選択肢が多い場合に有効
- **推奨手法：A2C（Advantage Actor-Critic）**
  - **理由**  
    - 賭け金の配分や複数馬券の組み合わせなど、連続的な選択肢を扱える
    - 方策（馬券購入ルール）を直接学習し、利益最大化を目指せる
    - 比較的シンプルで安定した学習が可能

---

## 3. アクター・クリティック型（Actor-Critic）  
**例：PPO, DDPG, SAC**

- **戦略例**
  - 「どの馬券をどれだけ買うか（アクター）」と「その選択がどれだけ良いか（クリティック）」を同時に学習
  - アクターが馬券購入戦略を提案し、クリティックがその戦略の価値を評価
  - より柔軟で高度な馬券購入戦略を構築可能
- **ポイント**
  - 状況に応じて賭け方を細かく調整できる
  - サンプル効率が高く、学習が安定しやすい
- **推奨手法：PPO（Proximal Policy Optimization）**
  - **理由**  
    - 柔軟な馬券購入戦略（賭け金の調整や複雑な組み合わせ）を構築できる
    - 学習が安定しやすく、実用例も多い
    - 状況に応じて賭け方を細かく調整可能

---

## 4. モデルベース型（Model-based）  
**例：Dreamer, MuZero**

- **戦略例**
  - 「レースの結果」や「オッズの変動」など、環境（競馬市場）のモデルを学習
  - 仮想的にレースをシミュレーションし、最適な馬券購入戦略を計画
  - 未知の状況や新しいレースにも柔軟に対応可能
- **ポイント**
  - データが少ない場合でも有効
  - 未来の展開を予測しながら戦略を立てられる
- **推奨手法：MuZero**
  - **理由**  
    - レース結果やオッズの変動など、競馬市場の複雑な環境をモデル化できる
    - 仮想的にレースをシミュレーションし、最適な戦略を計画できる
    - データが少ない場合や未知の状況にも強い

---

## まとめ表

| 手法             | 競馬での活用例                     | 戦略の特徴                       |
|------------------|------------------------------------|----------------------------------|
| 値関数型         | 最適な馬券種の選択                 | 過去データから儲かるパターン学習 |
| 方策型           | 賭け金や組み合わせの最適化         | 連続的な賭け方に強い             |
| アクター・クリティック型 | 柔軟な馬券購入戦略の構築         | 状況に応じて賭け方を調整         |
| モデルベース型   | レース結果や市場のシミュレーション | 未来予測を活かした戦略           |



# 実行コマンド

```
// dqn
python3 sanshutsu_kun/1_predict_models/a_value_based_dqn/dqn_keiba_simple.py
```






# リファクタコメント

```
5つの学習モデルの実行ファイルで共通の処理はmodulesにまとめて

DBとの接続処理
resultsへの実行ファイルのコピー、グラフ、GPTでの評価結果、作成したモデルの保存
ENVの読み込み

すでにmoduleになっているものはそのままでいい



またどのモデル作成の処理も以下の手順通りに実行されること

実行直後に実行ファイルをコピーしてresultsに入れること
特徴量の抽出やモデル作成の処理（ここはそれぞれのモデルで異なるはず）
結果が出たらグラフ作って、GPTにその画像と結果を送ってアドバイスをもらう


また
それぞれの処理が始まる前にprintで始まることを表示すること
大規模なforを回す処理はprint_progress_barで進捗表示できるようにすること

```